{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "from data_utils import MyDataset, my_collate, load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-chinese')\n",
    "base_model = AutoModel.from_pretrained('bert-base-chinese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader, test_dataloader = load_data(\"tnews\", \"./data\", tokenizer, 2, 2, \"bert\", \"dualcl\", workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'input_ids': tensor([[ 101, 3125, 3152, 2031,  860, 6568, 2791, 6756, 3136, 4906, 1092, 3180,\n",
       "            686, 5500, 1093, 3952,  102, 4905, 2094, 1762,  784,  720, 3340,  816,\n",
       "            678, 3291, 2159, 3211, 1355, 5715, 8043,  102,    0,    0,    0,    0,\n",
       "              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "              0],\n",
       "          [ 101, 3125, 3152, 2031,  860, 6568, 2791, 6756, 3136, 4906, 1092, 3180,\n",
       "            686, 5500, 1093, 3952,  102,  697, 1920, 6381, 2497, 8013, 3419, 3360,\n",
       "           2768, 1235, 1894, 1325, 1380, 5018,  671,  782, 8024,  924, 5384, 7439,\n",
       "           1927, 6428, 2130, 2768, 5468, 4673,  125,  121, 2399,  671, 1896,  715,\n",
       "            102]]),\n",
       "  'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0]]),\n",
       "  'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1]]),\n",
       "  'position_ids': tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  2,\n",
       "            3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n",
       "           21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33],\n",
       "          [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  2,\n",
       "            3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n",
       "           21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33]])},\n",
       " tensor([9, 3])]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = next(iter(train_dataloader))\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, labels = example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 101, 3125, 3152, 2031,  860, 6568, 2791, 6756, 3136, 4906, 1092, 3180,\n",
       "          686, 5500, 1093, 3952,  102, 4905, 2094, 1762,  784,  720, 3340,  816,\n",
       "          678, 3291, 2159, 3211, 1355, 5715, 8043,  102,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0],\n",
       "        [ 101, 3125, 3152, 2031,  860, 6568, 2791, 6756, 3136, 4906, 1092, 3180,\n",
       "          686, 5500, 1093, 3952,  102,  697, 1920, 6381, 2497, 8013, 3419, 3360,\n",
       "         2768, 1235, 1894, 1325, 1380, 5018,  671,  782, 8024,  924, 5384, 7439,\n",
       "         1927, 6428, 2130, 2768, 5468, 4673,  125,  121, 2399,  671, 1896,  715,\n",
       "          102]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] 故 文 娱 体 财 房 车 教 科 军 旅 世 股 农 游 [SEP] 种 子 在 什 么 条 件 下 更 容 易 发 芽 ？ [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "token_list = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "print(\" \".join(token_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] tensor(0) tensor(0) tensor(1)\n",
      "故 tensor(0) tensor(0) tensor(1)\n",
      "文 tensor(0) tensor(0) tensor(1)\n",
      "娱 tensor(0) tensor(0) tensor(1)\n",
      "体 tensor(0) tensor(0) tensor(1)\n",
      "财 tensor(0) tensor(0) tensor(1)\n",
      "房 tensor(0) tensor(0) tensor(1)\n",
      "车 tensor(0) tensor(0) tensor(1)\n",
      "教 tensor(0) tensor(0) tensor(1)\n",
      "科 tensor(0) tensor(0) tensor(1)\n",
      "军 tensor(0) tensor(0) tensor(1)\n",
      "旅 tensor(0) tensor(0) tensor(1)\n",
      "世 tensor(0) tensor(0) tensor(1)\n",
      "股 tensor(0) tensor(0) tensor(1)\n",
      "农 tensor(0) tensor(0) tensor(1)\n",
      "游 tensor(0) tensor(0) tensor(1)\n",
      "[SEP] tensor(1) tensor(0) tensor(1)\n",
      "种 tensor(2) tensor(0) tensor(1)\n",
      "子 tensor(3) tensor(0) tensor(1)\n",
      "在 tensor(4) tensor(0) tensor(1)\n",
      "什 tensor(5) tensor(0) tensor(1)\n",
      "么 tensor(6) tensor(0) tensor(1)\n",
      "条 tensor(7) tensor(0) tensor(1)\n",
      "件 tensor(8) tensor(0) tensor(1)\n",
      "下 tensor(9) tensor(0) tensor(1)\n",
      "更 tensor(10) tensor(0) tensor(1)\n",
      "容 tensor(11) tensor(0) tensor(1)\n",
      "易 tensor(12) tensor(0) tensor(1)\n",
      "发 tensor(13) tensor(0) tensor(1)\n",
      "芽 tensor(14) tensor(0) tensor(1)\n",
      "？ tensor(15) tensor(0) tensor(1)\n",
      "[SEP] tensor(16) tensor(0) tensor(1)\n",
      "[PAD] tensor(17) tensor(0) tensor(0)\n",
      "[PAD] tensor(18) tensor(0) tensor(0)\n",
      "[PAD] tensor(19) tensor(0) tensor(0)\n",
      "[PAD] tensor(20) tensor(0) tensor(0)\n",
      "[PAD] tensor(21) tensor(0) tensor(0)\n",
      "[PAD] tensor(22) tensor(0) tensor(0)\n",
      "[PAD] tensor(23) tensor(0) tensor(0)\n",
      "[PAD] tensor(24) tensor(0) tensor(0)\n",
      "[PAD] tensor(25) tensor(0) tensor(0)\n",
      "[PAD] tensor(26) tensor(0) tensor(0)\n",
      "[PAD] tensor(27) tensor(0) tensor(0)\n",
      "[PAD] tensor(28) tensor(0) tensor(0)\n",
      "[PAD] tensor(29) tensor(0) tensor(0)\n",
      "[PAD] tensor(30) tensor(0) tensor(0)\n",
      "[PAD] tensor(31) tensor(0) tensor(0)\n",
      "[PAD] tensor(32) tensor(0) tensor(0)\n",
      "[PAD] tensor(33) tensor(0) tensor(0)\n"
     ]
    }
   ],
   "source": [
    "for a, b, c, d in zip(token_list, inputs[\"position_ids\"][0], inputs[\"token_type_ids\"][0], inputs[\"attention_mask\"][0]):\n",
    "    print(a, b, c, d)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
